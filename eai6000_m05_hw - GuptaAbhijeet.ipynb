{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this assignment, we use House Dataset - \n",
    "\n",
    "### https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Regression Techniques to predict House Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train_data = pd.read_csv(\"House Price Prediction\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "After importing train and test data, we try to understand the data that we are dealing with using below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_variables(dataset):\n",
    "    print(\"Datatype = \" +str(type(dataset))+\"\\n\") \n",
    "    print(\"Data Shape = \"+str(dataset.shape)+\"\\n\")\n",
    "    print(\"Top5 Rows : \\n\\n\"+str(dataset.head())+\"\\n\\n\")\n",
    "    print(\"Data Columns:\\n\"+str(dataset.columns)+\"\\n\\n\")\n",
    "    print(\"No.of unique values :\\n\\n\"+str(dataset.nunique(axis=0).sort_values())+\"\\n\\n\")\n",
    "    print(\"Description :\\n\\n\"+str(dataset.describe())+\"\\n\\n\")\n",
    "    \n",
    "    #print(dataset.describe(exclude=[np.number]))\n",
    "    #Since no categorical variables, no need to have the above line\n",
    "    \n",
    "    print(\"Null count :\\n\\n\"+str(dataset.isnull().sum().sort_values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype = <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Data Shape = (1460, 81)\n",
      "\n",
      "Top5 Rows : \n",
      "\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "\n",
      "\n",
      "Data Columns:\n",
      "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
      "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
      "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
      "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
      "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
      "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
      "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
      "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
      "       'SaleCondition', 'SalePrice'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "No.of unique values :\n",
      "\n",
      "CentralAir         2\n",
      "Utilities          2\n",
      "Street             2\n",
      "Alley              2\n",
      "BsmtHalfBath       3\n",
      "                ... \n",
      "1stFlrSF         753\n",
      "BsmtUnfSF        780\n",
      "GrLivArea        861\n",
      "LotArea         1073\n",
      "Id              1460\n",
      "Length: 81, dtype: int64\n",
      "\n",
      "\n",
      "Description :\n",
      "\n",
      "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
      "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
      "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
      "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
      "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
      "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
      "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
      "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
      "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
      "\n",
      "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
      "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
      "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
      "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
      "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
      "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
      "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
      "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
      "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
      "\n",
      "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
      "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
      "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
      "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
      "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
      "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
      "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
      "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
      "\n",
      "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
      "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
      "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
      "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
      "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
      "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
      "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
      "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
      "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n",
      "\n",
      "\n",
      "Null count :\n",
      "\n",
      "Id                  0\n",
      "TotalBsmtSF         0\n",
      "Heating             0\n",
      "SaleCondition       0\n",
      "CentralAir          0\n",
      "                 ... \n",
      "FireplaceQu       690\n",
      "Fence            1179\n",
      "Alley            1369\n",
      "MiscFeature      1406\n",
      "PoolQC           1453\n",
      "Length: 81, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "understand_variables(house_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from data :\n",
    "\n",
    "1) We have multiple columns that have discrete string values such as Street etc.. - OneHot Encoding can convert strings to numeric form for ML models to process and train on\n",
    "\n",
    "2) We have plenty of columns with null values - we need to impute them - we will use mean for numeric columns, mode for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id is data index, and is not meant for training a model\n",
    "house_train_data = house_train_data.set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus, what we do next is split our columns into categorical (discrete - typically string) and numeric (continuous - typically decimals)\n",
    "\n",
    "categorical=[feature for feature in house_train_data.columns if house_train_data[feature].dtype=='O'] \n",
    "numerical=[feature for feature in house_train_data.columns if house_train_data[feature].dtype!='O' and feature!='SalePrice']\n",
    "\n",
    "# Then, we separate the target variable from fearures \n",
    "\n",
    "features = numerical + categorical\n",
    "target = ['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "Next, we create an ML Pipeline to process our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy='mean')), # replaces nulls in numeric columns with mean of available values\n",
    "           ('scaler', StandardScaler())]) # scales numeric data in range of -1 to 1\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy='most_frequent')), # replaces nulls in numeric columns with mode  of available values\n",
    "           ('onehot', OneHotEncoder(handle_unknown='ignore'))]) # converts categorical variables into a form suitable for ML algorithms\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numerical_transformer, numerical), # to process numerical columns\n",
    "                  ('cat', categorical_transformer, categorical)]) # to process categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def accuracy_per_regression_model(regression_model,cv=5):\n",
    "    \n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),('model', regression_model)]) # initiates preprocessing, then modelling\n",
    "\n",
    "    pipe.fit(house_train_data[features], np.ravel(house_train_data[target])) # trains data\n",
    "    \n",
    "    cv = cv # cross validation \n",
    "\n",
    "    error_percentage = (-cross_val_score(pipe, house_train_data[features],np.ravel(house_train_data[target]), cv=cv,scoring=\"neg_mean_absolute_error\").mean())/house_train_data[target].mean()*100\n",
    "    accuracy_percentage = 100 - error_percentage\n",
    "    \n",
    "    # error % = mean aboslute error of predicted prices vs actual prices / mean of actual prices * 100\n",
    "    # accuracy = 100 - error\n",
    "    \n",
    "    return accuracy_percentage.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try out Linear, Random Forest and XGB Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression accuracy : 89.60502768602375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor accuracy : 89.4800797362218\n",
      "XGBRegressor accuracy : 90.40758463471842\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearRegression accuracy : \"+str(accuracy_per_regression_model(LinearRegression())))\n",
    "print(\"RandomForestRegressor accuracy : \"+str(accuracy_per_regression_model(RandomForestRegressor(random_state=1))))\n",
    "print(\"XGBRegressor accuracy : \"+str(accuracy_per_regression_model(XGBRegressor(random_state=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost seems to fit best amongst all Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we will go ahead with XGBoostRegressor, and try to improve its accuracy further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used cross validation directly as 5. Now, we will use KFold Cross Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.40758463471842"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "accuracy_per_regression_model(XGBRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy has not changed with shuffle=True. So, we will keep it as false\n",
    "\n",
    "## Best Value for n_splits in KFold Cross Validation\n",
    "\n",
    "Next, we will find out the best value for n_splits from a range [2,5,10,15,20,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 89.70878951626808, 20: 90.33706074403274, 5: 90.40758463471842, 100: 90.50864423275947, 15: 90.5240708964742, 10: 90.54618703166244, 50: 90.61426574495266}\n"
     ]
    }
   ],
   "source": [
    "n_accuracy_dict = dict()\n",
    "\n",
    "for n in [2,5,10,15,20,50,100]:\n",
    "    cv = KFold(n_splits=n, random_state=1, shuffle=False)\n",
    "    \n",
    "    n_accuracy_dict[n] = accuracy_per_regression_model(XGBRegressor(),cv)\n",
    "\n",
    "print(dict(sorted(n_accuracy_dict.items(), key=lambda item: item[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_splits = 50 seems to provide best score. \n",
    "\n",
    "This is why we will choose that as n_splits for xgbRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated k-Fold Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go for Repeated k-Fold Cross Validation \n",
    "\n",
    "The estimate of model performance via k-fold cross-validation can be noisy.\n",
    "\n",
    "This means that each time the procedure is run, a different split of the dataset into k-folds can be implemented, and in turn, the distribution of performance scores can be different, resulting in a different mean estimate of model performance.\n",
    "\n",
    "One solution to reduce the noise in the estimated model performance is to increase the k-value. This will reduce the bias in the modelâ€™s estimated performance, although it will increase the variance: e.g. tie the result more to the specific dataset used in the evaluation.\n",
    "\n",
    "An alternate approach is to repeat the k-fold cross-validation process multiple times and report the mean performance across all folds and all repeats. This approach is generally referred to as repeated k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.62046434585686"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv = RepeatedKFold(n_splits=50, n_repeats=3, random_state=1)\n",
    "\n",
    "accuracy_per_regression_model(XGBRegressor(),cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model accuracy has improved a bit. But, we aren't sure with n_repeats = 3 provides the best accucracy. Thus, we search for the best n_repeats from a range of values [1,3,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 90.56938217777602\n",
      "3 : 90.62046434585686\n",
      "5 : 90.6529924008154\n",
      "10 : 90.69408668747397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "for n in [1,3,5,10]:\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=50, n_repeats=n, random_state=1)\n",
    "\n",
    "    print(str(n)+\" : \"+str(accuracy_per_regression_model(XGBRegressor(),cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result is for N_repeats = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further improvements on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to tune hyperparameters of the ML algorithm. One of these is called max_depth - The maximum depth of each tree, often values are between 1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_list = range(1,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 89.57777521119488\n",
      "2 : 90.9839599873666\n",
      "3 : 90.91137393263479\n",
      "4 : 90.8283626675284\n",
      "5 : 90.85393923389182\n"
     ]
    }
   ],
   "source": [
    "for depth in depth_list:\n",
    "    print(str(depth)+\" : \"+str(accuracy_per_regression_model(XGBRegressor(max_depth=depth),cv=RepeatedKFold(n_splits=50, n_repeats=10, random_state=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth seems to be the best for a value of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make use of pipelines to process input data. \n",
    "\n",
    "Then, we try to find out which ML algorithm offers most accuracy, which is XGBoostRegression. \n",
    "\n",
    "Then, we apply RepeatedKFold Cross Validation to find the most appropriate way to measure accuracy\n",
    "\n",
    "Finally, we try hyperparameter tuning to find the best parameters that enhance accuarcy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
